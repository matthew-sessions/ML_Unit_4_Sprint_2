{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "## *Data Science Unit 4 Sprint 2*\n",
    "\n",
    "# Sprint Challenge - Neural Network Foundations\n",
    "\n",
    "Table of Problems\n",
    "\n",
    "1. [Defining Neural Networks](#Q1)\n",
    "2. [Chocolate Gummy Bears](#Q2)\n",
    "    - Perceptron\n",
    "    - Multilayer Perceptron\n",
    "4. [Keras MMP](#Q3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Q1\"></a>\n",
    "## 1. Define the following terms:\n",
    "\n",
    "- **Neuron:**\n",
    "A basic unit that carries a single number.\n",
    "- **Input Layer:**\n",
    "Input layers are the features of the dataset.\n",
    "- **Hidden Layer:**\n",
    "Hidden layers are in-between the input and output that preform tasks on the neurons.\n",
    "- **Output Layer:**\n",
    "Output layer is the final layer of a neural net that delivers the result/s.\n",
    "- **Activation:**\n",
    "An activation function computs the neuron to determine the weights.\n",
    "- **Backpropagation:**\n",
    "A mechanism that tell the neural net if a prediction was correct or not. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chocolate Gummy Bears <a id=\"Q2\"></a>\n",
    "\n",
    "Right now, you're probably thinking, \"yuck, who the hell would eat that?\". Great question. Your candy company wants to know too. And you thought I was kidding about the [Chocolate Gummy Bears](https://nuts.com/chocolatessweets/gummies/gummy-bears/milk-gummy-bears.html?utm_source=google&utm_medium=cpc&adpos=1o1&gclid=Cj0KCQjwrfvsBRD7ARIsAKuDvMOZrysDku3jGuWaDqf9TrV3x5JLXt1eqnVhN0KM6fMcbA1nod3h8AwaAvWwEALw_wcB). \n",
    "\n",
    "Let's assume that a candy company has gone out and collected information on the types of Halloween candy kids ate. Our candy company wants to predict the eating behavior of witches, warlocks, and ghosts -- aka costumed kids. They shared a sample dataset with us. Each row represents a piece of candy that a costumed child was presented with during \"trick\" or \"treat\". We know if the candy was `chocolate` (or not chocolate) or `gummy` (or not gummy). Your goal is to predict if the costumed kid `ate` the piece of candy. \n",
    "\n",
    "If both chocolate and gummy equal one, you've got a chocolate gummy bear on your hands!?!?!\n",
    "![Chocolate Gummy Bear](https://ed910ae2d60f0d25bcb8-80550f96b5feb12604f4f720bfefb46d.ssl.cf1.rackcdn.com/3fb630c04435b7b5-2leZuM7_-zoom.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "candy = pd.read_csv('chocolate_gummy_bears.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chocolate</th>\n",
       "      <th>gummy</th>\n",
       "      <th>ate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   chocolate  gummy  ate\n",
       "0          0      1    1\n",
       "1          1      0    1\n",
       "2          0      1    1\n",
       "3          0      0    0\n",
       "4          1      1    0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candy.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron\n",
    "\n",
    "To make predictions on the `candy` dataframe. Build and train a Perceptron using numpy. Your target column is `ate` and your features: `chocolate` and `gummy`. Do not do any feature engineering. :P\n",
    "\n",
    "Once you've trained your model, report your accuracy. You will not be able to achieve more than ~50% with the simple perceptron. Explain why you could not achieve a higher accuracy with the *simple perceptron* architecture, because it's possible to achieve ~95% accuracy on this dataset. Provide your answer in markdown (and *optional* data anlysis code) after your perceptron implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "# Start your candy perceptron here\n",
    "\n",
    "X = candy[['chocolate', 'gummy']].values\n",
    "y = candy['ate'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivate(x):\n",
    "    sx = sigmoid(x)\n",
    "    return( sx * (1-sx))\n",
    "\n",
    "class Perceptron(object):\n",
    "    \n",
    "    def __init__(self, niter = 1):\n",
    "        self.niter = niter\n",
    "    \n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        inputs = X\n",
    "        correct_outputs = y.reshape(-1,1)\n",
    "\n",
    "        weights = 2 * np.random.random((X.shape[1],1)) - 1\n",
    "       # bias = 2 * np.random.random((len(X),1)) - 1\n",
    "        \n",
    "        for i in range(self.niter):\n",
    "\n",
    "            weighted_sum = np.dot(inputs, weights)# + bias\n",
    "\n",
    "            activated_output = sigmoid(weighted_sum)\n",
    "\n",
    "   \n",
    "            error = correct_outputs - activated_output\n",
    "\n",
    "            adjustments = error * sigmoid_derivate(activated_output)\n",
    "\n",
    "  \n",
    "            #bias += error\n",
    "            weights += np.dot(inputs.T, adjustments)\n",
    "        self.activated_output = activated_output\n",
    "        self.weights = weights\n",
    "        #self.bias = bias\n",
    "        \n",
    "\n",
    "    def predict(self, X):\n",
    "        weighted_sum = np.dot(X, self.weights) #+ self.bias\n",
    "        \n",
    "        return sigmoid(weighted_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = Perceptron()\n",
    "nn.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = nn.predict(X)\n",
    "\n",
    "n_pred = []\n",
    "for i in pred:\n",
    "    n_pred.append(int(i))\n",
    "from sklearn.metrics import accuracy_score as acs\n",
    "\n",
    "acs(y, n_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 50% accuracy score\n",
    "\n",
    "A simple percoptron is not able to preform well becuase it does not have a mechanism to update the weights effectively. Also this model does not implemtent bias witch prohibts the the model from learning the weights. If bias is implemtened, the accuracy score jumps to 75%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron <a id=\"Q3\"></a>\n",
    "\n",
    "Using the sample candy dataset, implement a Neural Network Multilayer Perceptron class that uses backpropagation to update the network's weights. Your Multilayer Perceptron should be implemented in Numpy. \n",
    "Your network must have one hidden layer.\n",
    "\n",
    "Once you've trained your model, report your accuracy. Explain why your MLP's performance is considerably better than your simple perceptron's on the candy dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "\n",
    "        self.inputs = 2\n",
    "        self.hiddenNodes = 3\n",
    "        self.outputNodes = 1\n",
    "\n",
    "\n",
    "        self.weights1 = np.random.rand(self.inputs, self.hiddenNodes)\n",
    "        self.weights2 = np.random.rand(self.hiddenNodes, self.outputNodes)\n",
    "        \n",
    "    def sigmoid(self, s):\n",
    "        return 1 / (1+np.exp(-s))\n",
    "    \n",
    "    def sigmoidPrime(self, s):\n",
    "        return s * (1 - s)\n",
    "    \n",
    "    def feed_forward(self, X):\n",
    "\n",
    "        self.hidden_sum = np.dot(X, self.weights1)\n",
    "\n",
    "        self.activated_hidden = self.sigmoid(self.hidden_sum)\n",
    "        self.output_sum = np.dot(self.activated_hidden, self.weights2)\n",
    "        self.activated_output = self.sigmoid(self.output_sum)\n",
    "        \n",
    "        return self.activated_output\n",
    "        \n",
    "    def backward(self, X,y,o):\n",
    "\n",
    "        self.o_error = y - o\n",
    "        self.o_delta = self.o_error * self.sigmoidPrime(o)\n",
    "        self.z2_error = self.o_delta.dot(self.weights2.T)\n",
    "        self.z2_delta = self.z2_error * self.sigmoidPrime(self.activated_hidden)\n",
    "        self.weights1 += X.T.dot(self.z2_delta)\n",
    "        self.weights2 += self.activated_hidden.T.dot(self.o_delta)\n",
    "        \n",
    "\n",
    "    def train(self, X, y):\n",
    "        o = self.feed_forward(X)\n",
    "        self.backward(X,y,o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNetwork()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [1],\n",
       "       ...,\n",
       "       [1],\n",
       "       [1],\n",
       "       [1]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------EPOCH 1---------+\n",
      "Input: \n",
      " [[0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " ...\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]]\n",
      "Actual Output: \n",
      " [[1]\n",
      " [1]\n",
      " [1]\n",
      " ...\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      "Predicted Output: \n",
      " [[0.49999985]\n",
      " [0.49999982]\n",
      " [0.49999985]\n",
      " ...\n",
      " [0.49999985]\n",
      " [0.49999985]\n",
      " [0.49999982]]\n",
      "Loss: \n",
      " 0.20115007378461938\n",
      "+---------EPOCH 2---------+\n",
      "Input: \n",
      " [[0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " ...\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]]\n",
      "Actual Output: \n",
      " [[1]\n",
      " [1]\n",
      " [1]\n",
      " ...\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      "Predicted Output: \n",
      " [[0.49999985]\n",
      " [0.49999982]\n",
      " [0.49999985]\n",
      " ...\n",
      " [0.49999985]\n",
      " [0.49999985]\n",
      " [0.49999982]]\n",
      "Loss: \n",
      " 0.2011500737709538\n",
      "+---------EPOCH 3---------+\n",
      "Input: \n",
      " [[0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " ...\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]]\n",
      "Actual Output: \n",
      " [[1]\n",
      " [1]\n",
      " [1]\n",
      " ...\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      "Predicted Output: \n",
      " [[0.49999985]\n",
      " [0.49999982]\n",
      " [0.49999985]\n",
      " ...\n",
      " [0.49999985]\n",
      " [0.49999985]\n",
      " [0.49999982]]\n",
      "Loss: \n",
      " 0.20115007375729324\n",
      "+---------EPOCH 4---------+\n",
      "Input: \n",
      " [[0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " ...\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]]\n",
      "Actual Output: \n",
      " [[1]\n",
      " [1]\n",
      " [1]\n",
      " ...\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      "Predicted Output: \n",
      " [[0.49999985]\n",
      " [0.49999982]\n",
      " [0.49999985]\n",
      " ...\n",
      " [0.49999985]\n",
      " [0.49999985]\n",
      " [0.49999982]]\n",
      "Loss: \n",
      " 0.20115007374363786\n",
      "+---------EPOCH 5---------+\n",
      "Input: \n",
      " [[0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " ...\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]]\n",
      "Actual Output: \n",
      " [[1]\n",
      " [1]\n",
      " [1]\n",
      " ...\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      "Predicted Output: \n",
      " [[0.49999985]\n",
      " [0.49999982]\n",
      " [0.49999985]\n",
      " ...\n",
      " [0.49999985]\n",
      " [0.49999985]\n",
      " [0.49999982]]\n",
      "Loss: \n",
      " 0.2011500737299875\n",
      "+---------EPOCH 6---------+\n",
      "Input: \n",
      " [[0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " ...\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]]\n",
      "Actual Output: \n",
      " [[1]\n",
      " [1]\n",
      " [1]\n",
      " ...\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      "Predicted Output: \n",
      " [[0.49999985]\n",
      " [0.49999982]\n",
      " [0.49999985]\n",
      " ...\n",
      " [0.49999985]\n",
      " [0.49999985]\n",
      " [0.49999982]]\n",
      "Loss: \n",
      " 0.2011500737163422\n",
      "+---------EPOCH 7---------+\n",
      "Input: \n",
      " [[0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " ...\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]]\n",
      "Actual Output: \n",
      " [[1]\n",
      " [1]\n",
      " [1]\n",
      " ...\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      "Predicted Output: \n",
      " [[0.49999985]\n",
      " [0.49999982]\n",
      " [0.49999985]\n",
      " ...\n",
      " [0.49999985]\n",
      " [0.49999985]\n",
      " [0.49999982]]\n",
      "Loss: \n",
      " 0.201150073702702\n",
      "+---------EPOCH 5000---------+\n",
      "Input: \n",
      " [[0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " ...\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]]\n",
      "Actual Output: \n",
      " [[1]\n",
      " [1]\n",
      " [1]\n",
      " ...\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      "Predicted Output: \n",
      " [[0.49999992]\n",
      " [0.49999991]\n",
      " [0.49999992]\n",
      " ...\n",
      " [0.49999992]\n",
      " [0.49999992]\n",
      " [0.49999991]]\n",
      "Loss: \n",
      " 0.20115003834589115\n",
      "+---------EPOCH 10000---------+\n",
      "Input: \n",
      " [[0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " ...\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]]\n",
      "Actual Output: \n",
      " [[1]\n",
      " [1]\n",
      " [1]\n",
      " ...\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      "Predicted Output: \n",
      " [[0.49999994]\n",
      " [0.49999994]\n",
      " [0.49999994]\n",
      " ...\n",
      " [0.49999994]\n",
      " [0.49999994]\n",
      " [0.49999994]]\n",
      "Loss: \n",
      " 0.2011500259143815\n"
     ]
    }
   ],
   "source": [
    "for i in range(10000):\n",
    "    if (i in [a for a in range(7)]) or ((i + 1) % 5000 == 0):\n",
    "        print('+' + '---' * 3 + f'EPOCH {i+1}' + '---'*3 + '+')\n",
    "        print('Input: \\n', X)\n",
    "        print('Actual Output: \\n', y.reshape(-1,1))\n",
    "        print('Predicted Output: \\n', str(nn.feed_forward(X)))\n",
    "        print(\"Loss: \\n\", str(np.mean(np.square(y.reshape(-1,1) - nn.feed_forward(X)))))\n",
    "    nn.train(X,y.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivate(x):\n",
    "    sx = sigmoid(x)\n",
    "    return( sx * (1-sx))\n",
    "\n",
    "class Perceptron(object):\n",
    "    \n",
    "    def __init__(self, niter = 500):\n",
    "        self.niter = niter\n",
    "    \n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        inputs = X\n",
    "        correct_outputs = y.reshape(-1,1)\n",
    "\n",
    "        weights = 2 * np.random.random((X.shape[1],1)) - 1\n",
    "        bias = 2 * np.random.random((len(X),1)) - 1\n",
    "        \n",
    "        for i in range(self.niter):\n",
    "\n",
    "            weighted_sum = np.dot(inputs, weights) + bias\n",
    "\n",
    "            activated_output = sigmoid(weighted_sum)\n",
    "\n",
    "   \n",
    "            error = correct_outputs - activated_output\n",
    "\n",
    "            adjustments = error * sigmoid_derivate(activated_output)\n",
    "\n",
    "  \n",
    "            bias += error\n",
    "            weights += np.dot(inputs.T, adjustments)\n",
    "        self.activated_output = activated_output\n",
    "        self.weights = weights\n",
    "        self.bias = bias\n",
    "        \n",
    "\n",
    "    def predict(self, X):\n",
    "        weighted_sum = np.dot(X, self.weights) + self.bias\n",
    "        \n",
    "        return sigmoid(weighted_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "nn = Perceptron()\n",
    "nn.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9859"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = nn.predict(X)\n",
    "n_pred = []\n",
    "for i in pred:\n",
    "    n_pred.append(int(i))\n",
    "\n",
    "acs(y,n_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backprop and bias leads to an accuracy score of +95%.\n",
    "\n",
    "Backpropagation allows the neural net to learn from each mistake to better determine the weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P.S. Don't try candy gummy bears. They're disgusting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Keras MMP <a id=\"Q3\"></a>\n",
    "\n",
    "Implement a Multilayer Perceptron architecture of your choosing using the Keras library. Train your model and report its baseline accuracy. Then hyperparameter tune at least two parameters and report your model's accuracy.\n",
    "Use the Heart Disease Dataset (binary classification)\n",
    "Use an appropriate loss function for a binary classification task\n",
    "Use an appropriate activation function on the final layer of your network.\n",
    "Train your model using verbose output for ease of grading.\n",
    "Use GridSearchCV or RandomSearchCV to hyperparameter tune your model. (for at least two hyperparameters)\n",
    "When hyperparameter tuning, show you work by adding code cells for each new experiment.\n",
    "Report the accuracy for each combination of hyperparameters as you test them so that we can easily see which resulted in the highest accuracy.\n",
    "You must hyperparameter tune at least 3 parameters in order to get a 3 on this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Four main hyperparameter focuses\n",
    "- Number of layers\n",
    "- Number of echos\n",
    "- batch_size\n",
    "- Number of nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(303, 14)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>249</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>144</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>66</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>160</td>\n",
       "      <td>228</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>138</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>66</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>160</td>\n",
       "      <td>246</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>59</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>140</td>\n",
       "      <td>221</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>164</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>124</td>\n",
       "      <td>255</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>175</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
       "270   46    1   0       120   249    0        0      144      0      0.8   \n",
       "150   66    1   0       160   228    0        0      138      0      2.3   \n",
       "247   66    1   1       160   246    0        1      120      1      0.0   \n",
       "77    59    1   1       140   221    0        1      164      1      0.0   \n",
       "90    48    1   2       124   255    1        1      175      0      0.0   \n",
       "\n",
       "     slope  ca  thal  target  \n",
       "270      2   0     3       0  \n",
       "150      2   0     1       1  \n",
       "247      1   3     1       0  \n",
       "77       2   0     2       1  \n",
       "90       2   2     2       1  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/heart.csv')\n",
    "df = df.sample(frac=1)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.92271345,  0.68100522, -0.93851463, ...,  0.97635214,\n",
       "        -0.71442887,  1.12302895],\n",
       "       [ 1.28306308,  0.68100522, -0.93851463, ...,  0.97635214,\n",
       "        -0.71442887, -2.14887271],\n",
       "       [ 1.28306308,  0.68100522,  0.03203122, ..., -0.64911323,\n",
       "         2.22410436, -2.14887271],\n",
       "       ...,\n",
       "       [-1.36386876,  0.68100522,  0.03203122, ...,  0.97635214,\n",
       "        -0.71442887, -0.51292188],\n",
       "       [-0.92271345,  0.68100522,  0.03203122, ...,  0.97635214,\n",
       "        -0.71442887,  1.12302895],\n",
       "       [-1.1432911 ,  0.68100522, -0.93851463, ..., -2.27457861,\n",
       "        -0.71442887, -2.14887271]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df.values[:,:-1]\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.target.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "def build_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_dim=13, activation='relu'))\n",
    "    model.add(Dense(1, activation='relu'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.6666666666666666 using {'batch_size': 10, 'epochs': 20}\n",
      "Means: 0.6666666666666666, Stdev: 0.10550736937169447 with: {'batch_size': 10, 'epochs': 20}\n",
      "Means: 0.5841584106286367, Stdev: 0.08440072713159942 with: {'batch_size': 20, 'epochs': 20}\n",
      "Means: 0.5709570944309235, Stdev: 0.060675823753857584 with: {'batch_size': 40, 'epochs': 20}\n",
      "Means: 0.528052826722463, Stdev: 0.009334740203990824 with: {'batch_size': 60, 'epochs': 20}\n",
      "Means: 0.4785478711128235, Stdev: 0.08719369697853102 with: {'batch_size': 80, 'epochs': 20}\n",
      "Means: 0.6204620401064554, Stdev: 0.12892426620220182 with: {'batch_size': 100, 'epochs': 20}\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'batch_size': [10, 20, 40, 60, 80, 100],\n",
    "              'epochs': [20]}\n",
    "\n",
    "model = KerasClassifier(build_fn=build_model, verbose=False, validation_split=0.10)\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1)\n",
    "grid_result = grid.fit(X, y)\n",
    "\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base line score of 66%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.7887788812319437 using {'batch_size': 10, 'epochs': 20}\n",
      "Means: 0.7887788812319437, Stdev: 0.03820407080172275 with: {'batch_size': 10, 'epochs': 20}\n",
      "Means: 0.7821782231330872, Stdev: 0.016168244308767975 with: {'batch_size': 20, 'epochs': 20}\n",
      "Means: 0.6963696380456289, Stdev: 0.15640917850366245 with: {'batch_size': 40, 'epochs': 20}\n",
      "Means: 0.6963696281115214, Stdev: 0.05382676587232076 with: {'batch_size': 60, 'epochs': 20}\n",
      "Means: 0.49504950642585754, Stdev: 0.0820449031203073 with: {'batch_size': 80, 'epochs': 20}\n",
      "Means: 0.6336633662382761, Stdev: 0.10876077367654916 with: {'batch_size': 100, 'epochs': 20}\n"
     ]
    }
   ],
   "source": [
    "def build_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_dim=13, activation='relu'))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(25, activation='relu'))\n",
    "    model.add(Dense(15, activation='relu'))\n",
    "    model.add(Dense(1, activation='relu'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = KerasClassifier(build_fn=build_model, verbose=False, validation_split=0.10)\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1)\n",
    "grid_result = grid.fit(X, y)\n",
    "\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Added layers as the first for the first tuned parameter. Confirmed batch_size of 10. Model increased to 78% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__abstractmethods__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_abc_cache',\n",
       " '_abc_negative_cache',\n",
       " '_abc_negative_cache_version',\n",
       " '_abc_registry',\n",
       " '_check_is_fitted',\n",
       " '_estimator_type',\n",
       " '_format_results',\n",
       " '_get_param_names',\n",
       " '_get_tags',\n",
       " '_required_parameters',\n",
       " '_run_search',\n",
       " 'best_estimator_',\n",
       " 'best_index_',\n",
       " 'best_params_',\n",
       " 'best_score_',\n",
       " 'classes_',\n",
       " 'cv',\n",
       " 'cv_results_',\n",
       " 'decision_function',\n",
       " 'error_score',\n",
       " 'estimator',\n",
       " 'fit',\n",
       " 'get_params',\n",
       " 'iid',\n",
       " 'inverse_transform',\n",
       " 'multimetric_',\n",
       " 'n_jobs',\n",
       " 'n_splits_',\n",
       " 'param_grid',\n",
       " 'pre_dispatch',\n",
       " 'predict',\n",
       " 'predict_log_proba',\n",
       " 'predict_proba',\n",
       " 'refit',\n",
       " 'refit_time_',\n",
       " 'return_train_score',\n",
       " 'score',\n",
       " 'scorer_',\n",
       " 'scoring',\n",
       " 'set_params',\n",
       " 'transform',\n",
       " 'verbose']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(grid_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([2.07969395, 1.75161465, 1.52807903, 1.50410398, 1.45364682,\n",
       "        1.46441897]),\n",
       " 'std_fit_time': array([0.15510651, 0.26054729, 0.19578866, 0.18349208, 0.26392152,\n",
       "        0.17134057]),\n",
       " 'mean_score_time': array([0.05069606, 0.05028526, 0.04780968, 0.04511476, 0.04577573,\n",
       "        0.04668721]),\n",
       " 'std_score_time': array([0.00265777, 0.00283199, 0.00419126, 0.00085364, 0.00402443,\n",
       "        0.00371894]),\n",
       " 'param_batch_size': masked_array(data=[10, 20, 40, 60, 80, 100],\n",
       "              mask=[False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_epochs': masked_array(data=[20, 20, 20, 20, 20, 20],\n",
       "              mask=[False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'batch_size': 10, 'epochs': 20},\n",
       "  {'batch_size': 20, 'epochs': 20},\n",
       "  {'batch_size': 40, 'epochs': 20},\n",
       "  {'batch_size': 60, 'epochs': 20},\n",
       "  {'batch_size': 80, 'epochs': 20},\n",
       "  {'batch_size': 100, 'epochs': 20}],\n",
       " 'split0_test_score': array([0.77227724, 0.78217822, 0.47524753, 0.65346533, 0.47524753,\n",
       "        0.6732673 ]),\n",
       " 'split1_test_score': array([0.84158415, 0.8019802 , 0.81188118, 0.66336632, 0.40594059,\n",
       "        0.74257427]),\n",
       " 'split2_test_score': array([0.75247526, 0.76237625, 0.8019802 , 0.77227724, 0.60396039,\n",
       "        0.48514852]),\n",
       " 'mean_test_score': array([0.78877888, 0.78217822, 0.69636964, 0.69636963, 0.49504951,\n",
       "        0.63366337]),\n",
       " 'std_test_score': array([0.03820407, 0.01616824, 0.15640918, 0.05382677, 0.0820449 ,\n",
       "        0.10876077]),\n",
       " 'rank_test_score': array([1, 2, 3, 4, 6, 5], dtype=int32)}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_result.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.8118811845779419 using {'batch_size': 10, 'epochs': 30}\n",
      "Means: 0.7722772359848022, Stdev: 0.008084122154383987 with: {'batch_size': 10, 'epochs': 20}\n",
      "Means: 0.8118811845779419, Stdev: 0.042777153577535476 with: {'batch_size': 10, 'epochs': 30}\n",
      "Means: 0.6402640243371328, Stdev: 0.12636428620113693 with: {'batch_size': 10, 'epochs': 45}\n",
      "Means: 0.7953795393308004, Stdev: 0.02034459460670113 with: {'batch_size': 10, 'epochs': 60}\n",
      "Means: 0.7788778940836588, Stdev: 0.03645332582644608 with: {'batch_size': 10, 'epochs': 80}\n",
      "Means: 0.7425742745399475, Stdev: 0.0370461016997341 with: {'batch_size': 10, 'epochs': 120}\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'batch_size': [10],\n",
    "              'epochs': [20,30,45,60,80,120]}\n",
    "\n",
    "model = KerasClassifier(build_fn=build_model, verbose=False, validation_split=0.10)\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1)\n",
    "grid_result = grid.fit(X, y)\n",
    "\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 30 epochs increased my score to 81%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_dim=13, activation='relu'))\n",
    "    model.add(Dense(12, activation='relu'))\n",
    "    model.add(Dense(12, activation='relu'))\n",
    "    model.add(Dense(12, activation='relu'))\n",
    "    model.add(Dense(12, activation='relu'))\n",
    "    model.add(Dense(12, activation='relu'))\n",
    "    model.add(Dense(12, activation='relu'))\n",
    "    model.add(Dense(12, activation='relu'))\n",
    "    model.add(Dense(12, activation='relu'))\n",
    "    model.add(Dense(12, activation='relu'))\n",
    "    model.add(Dense(1, activation='relu'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def test_model():\n",
    "    param_grid = {'batch_size': [10,20,50,60],\n",
    "                  'epochs': [30,150,200]}\n",
    "    \n",
    "    model = KerasClassifier(build_fn=build_model, verbose=False, validation_split=0.10)\n",
    "\n",
    "    grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1)\n",
    "    grid_result = grid.fit(X, y)\n",
    "\n",
    "    print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "    means = grid_result.cv_results_['mean_test_score']\n",
    "    stds = grid_result.cv_results_['std_test_score']\n",
    "    params = grid_result.cv_results_['params']\n",
    "    for mean, stdev, param in zip(means, stds, params):\n",
    "        print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.7854785521825155 using {'batch_size': 20, 'epochs': 150}\n",
      "Means: 0.5643564363320669, Stdev: 0.1711086648947219 with: {'batch_size': 10, 'epochs': 30}\n",
      "Means: 0.7524752616882324, Stdev: 0.021388576788767738 with: {'batch_size': 10, 'epochs': 150}\n",
      "Means: 0.7623762488365173, Stdev: 0.03523787151819811 with: {'batch_size': 10, 'epochs': 200}\n",
      "Means: 0.5874587496121725, Stdev: 0.15174341516061454 with: {'batch_size': 20, 'epochs': 30}\n",
      "Means: 0.7854785521825155, Stdev: 0.04148449288410131 with: {'batch_size': 20, 'epochs': 150}\n",
      "Means: 0.4554455478986104, Stdev: 0.0352378854744143 with: {'batch_size': 20, 'epochs': 200}\n",
      "Means: 0.6798679927984873, Stdev: 0.13916282954842815 with: {'batch_size': 50, 'epochs': 30}\n",
      "Means: 0.6897689799467722, Stdev: 0.1543058664364082 with: {'batch_size': 50, 'epochs': 150}\n",
      "Means: 0.6963696380456289, Stdev: 0.15281632439934778 with: {'batch_size': 50, 'epochs': 200}\n",
      "Means: 0.6171617209911346, Stdev: 0.12162063284890626 with: {'batch_size': 60, 'epochs': 30}\n",
      "Means: 0.5709570944309235, Stdev: 0.18028382895609188 with: {'batch_size': 60, 'epochs': 150}\n",
      "Means: 0.6303630371888479, Stdev: 0.11035150594736975 with: {'batch_size': 60, 'epochs': 200}\n"
     ]
    }
   ],
   "source": [
    "test_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "nteract": {
   "version": "0.15.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
